{"cells":[{"cell_type":"markdown","metadata":{"id":"4TagrXNO6h2X"},"source":["---\n","\n","# IMPORTANT\n","\n","**Please remember to save this notebook `dropout.ipynb` and `layers.py` as you work on them!**\n","\n","---"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9txh_Q_G6h2b","executionInfo":{"status":"ok","timestamp":1686984233035,"user_tz":-480,"elapsed":3807,"user":{"displayName":"Kevin Tsai","userId":"03189192575775510032"}},"outputId":"a19c0c58-e0ca-4e8a-c5bd-c278ea8f48ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks/SC201L13/sc201/datasets\n","/content\n"]}],"source":["# this mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# 請輸入 SC201L13 資料夾之所在位置\n","FOLDERNAME = 'Colab\\ Notebooks/SC201L13'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/MyDrive/{}'.format(FOLDERNAME))\n","\n","# this downloads the CIFAR-10 dataset to your Drive\n","# if it doesn't already exist.\n","%cd drive/MyDrive/$FOLDERNAME/sc201/datasets/\n","!bash get_datasets.sh\n","%cd /content"]},{"cell_type":"code","execution_count":2,"metadata":{"scrolled":false,"tags":["pdf-ignore"],"colab":{"base_uri":"https://localhost:8080/"},"id":"_fYRMTvH6h2c","executionInfo":{"status":"ok","timestamp":1686984233035,"user_tz":-480,"elapsed":3,"user":{"displayName":"Kevin Tsai","userId":"03189192575775510032"}},"outputId":"dc198bae-8b4c-4591-c1db-9761f116097e"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/SC201L13\n"]}],"source":["# As usual, a bit of setup\n","%cd drive/MyDrive/$FOLDERNAME\n","from __future__ import print_function\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sc201.classifiers.fc_net import *\n","from sc201.data_utils import get_CIFAR10_data\n","from sc201.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n","from sc201.solver import Solver\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2\n","\n","def rel_error(x, y):\n","  \"\"\" returns relative error \"\"\"\n","  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"]},{"cell_type":"code","execution_count":3,"metadata":{"tags":["pdf-ignore"],"colab":{"base_uri":"https://localhost:8080/"},"id":"ncoEwoC56h2d","executionInfo":{"status":"ok","timestamp":1686984239573,"user_tz":-480,"elapsed":6540,"user":{"displayName":"Kevin Tsai","userId":"03189192575775510032"}},"outputId":"fe49f86b-d43c-413b-8df7-fc3505b3a6d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["X_train:  (49000, 3, 32, 32)\n","y_train:  (49000,)\n","X_val:  (1000, 3, 32, 32)\n","y_val:  (1000,)\n","X_test:  (1000, 3, 32, 32)\n","y_test:  (1000,)\n"]}],"source":["# Load the (preprocessed) CIFAR10 data.\n","\n","data = get_CIFAR10_data()\n","for k, v in data.items():\n","  print('%s: ' % k, v.shape)"]},{"cell_type":"markdown","metadata":{"id":"b5-FtzbU6h2d"},"source":["# Modular Design of Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"DUULo-Iu6h2e"},"source":["大家在上一份作業裡寫出了一套 fully-connected two-layer neural network，這套網路的架構算是比較簡單，可以透由單一函數來計算 loss 與 gradients。不過要能夠建立更複雜更多層的神經網路，我們必須將網路的架構模組化，將每一層分開定義，再將其拼湊在一起，組合成不同的 network。\n","\n","模組化的原則如下：\n","\n","我們分開定義每一種 layer 類型（如 affine、relu、normalization、dropout、softmax），為每一層定義 `forward` 及 `backward` 函數。`forward` 函數會接收前一層的輸出結果和本層要使用的權重等參數，計算並回傳本層的 output，也同時將 backward pass 計算梯度會需要的資訊儲存在一個 cache 中並將其回傳：\n","\n","```python\n","def layer_forward(x, p):\n","  \"\"\" Receive inputs x and parameters p \"\"\"\n","  # Do some computations ...\n","  z = # ... some intermediate value\n","  # Do some more computations ...\n","  out = # the output\n","\n","  cache = (x, p, z, out) # Values we need to compute gradients\n","\n","  return out, cache\n","```\n","\n","而 `backward` 函數會接收 cost function 對於下一層的梯度以及 forward pass 回傳的 `cache` 物件，計算並回傳 cost function 對於本層的梯度。\n","\n","```python\n","def layer_backward(dout, cache):\n","  \"\"\"\n","  Receive dout (derivative of loss with respect to outputs) and cache,\n","  and compute derivative with respect to inputs.\n","  \"\"\"\n","  # Unpack cache values\n","  x, p, z, out = cache\n","\n","  # Use values in cache to compute derivatives\n","  dx = # Derivative of loss with respect to x\n","  dp = # Derivative of loss with respect to p\n","\n","  return dx, dp\n","```\n","\n","有了這些 layer，要怎麼組合出完整的 neural network？以作業4的 `TwoLayerNet` 為例，它的架構就會是 affine - relu - affine - softmax。我們會在神經網路初始化時定義它的權重 W1、W2 及偏差 b1、b2。\n","\n","執行 forward pass 的時候，我們將 input X (圖像的畫素) 丟入第一層 affine layer 的 affine_forward 函數，然後將這一層的輸出結果丟入 relu_forward，以此類推，直到最後用 softmax_forward 及 input 的 true label y 計算出網路的預測與正確標籤之間的 cost。\n","\n","`affine_forward(X, W1, b1)` $\\rightarrow$ `relu_forward(...)` $\\rightarrow$ `affine_forward(..., W2, b2)`  $\\rightarrow$ `softmax_forward(..., y)` $\\rightarrow$ $J$\n","\n","這個流程中產生的所有 cache 我們會先儲存起來。執行 backward pass 的時候，我們會將 cost 以及 softmax 在 forward pass 產生的 cache 丟入 `softmax_backward`。這個函數輸出的結果會是 cost function 對於第二層 affine layer 的 output 之梯度。我們再將這個梯度及第二層 affine layer 的 cache 丟入 `affine_backward`，得到 cost function 對於 relu 結果、W2 和 b2的梯度，以此類推，最後計算出 cost function 對於所有參數 W1、b1、W2和 b2 的梯度。這些梯度就可以拿來做 gradient descent 的 update。\n","\n","$J$ $\\rightarrow$ `softmax_backward(..., cache_softmax)` $\\rightarrow$ `affine_backward(..., cache_affine2)` $\\rightarrow$ `relu_backward(..., cache_relu)` (and $dJ/dW2$, $dJ/db2$) $\\rightarrow$ `affine_backward(..., cache_affine1)` $\\rightarrow$ $dJ/dX$ (and $dJ/dW1$, $dJ/db1$)\n","\n","最後要使用模型做預測時，我們可以執行 forward pass，在計算完第二層 `affine_forward` 後提前終止，找出這層輸出的 class scores 中最高的對應類別。\n","\n","要建立更複雜的 neural network 的話，我們只需將適當的 `forward` 及 `backward` 函數插入上方流程即可！"]},{"cell_type":"markdown","metadata":{"id":"T872eq8V6h2f"},"source":["# Example module with `affine`"]},{"cell_type":"markdown","metadata":{"id":"KZak-KNK6h2f"},"source":["請打開 `layers.py` 並詳讀 `affine_forward` 及 `affine_backward` 函數，以了解其運作原理。"]},{"cell_type":"markdown","metadata":{"id":"mdZCqvZI6h2g"},"source":["# ReLU activation: forward\n","請寫出 `layers.py` 檔案中的 `relu_forward` 函數，然後執行下方 code 以做檢查"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qon_9pS96h2h","executionInfo":{"status":"ok","timestamp":1686984239573,"user_tz":-480,"elapsed":16,"user":{"displayName":"Kevin Tsai","userId":"03189192575775510032"}},"outputId":"51ccfaca-a5d0-4b89-8b34-cf82e3d8c00b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Testing relu_forward function:\n","difference:  4.999999798022158e-08\n"]}],"source":["# Test the relu_forward function\n","\n","x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n","\n","out, _ = relu_forward(x)\n","correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n","                        [ 0.,          0.,          0.04545455,  0.13636364,],\n","                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n","\n","# Compare your output with ours. The error should be on the order of e-8\n","print('Testing relu_forward function:')\n","print('difference: ', rel_error(out, correct_out))"]},{"cell_type":"markdown","metadata":{"id":"rJCYDRyA6h2h"},"source":["# ReLU activation: backward\n","請完成同個檔案中的 `relu_backward` 函數，然後執行下方 code 以檢查 gradient 數值。"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"TcQpvLTJ6h2h","executionInfo":{"status":"ok","timestamp":1686984239573,"user_tz":-480,"elapsed":13,"user":{"displayName":"Kevin Tsai","userId":"03189192575775510032"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fd47c248-e4ed-423b-fdcb-5e74a06738d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Testing relu_backward function:\n","dx error:  3.2756349136310288e-12\n"]}],"source":["np.random.seed(231)\n","x = np.random.randn(10, 10)\n","dout = np.random.randn(*x.shape)\n","\n","dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n","\n","_, cache = relu_forward(x)\n","dx = relu_backward(dout, cache)\n","\n","# The error should be on the order of e-12\n","print('Testing relu_backward function:')\n","print('dx error: ', rel_error(dx_num, dx))"]},{"cell_type":"markdown","metadata":{"tags":["pdf-title"],"id":"12plMFP16h2h"},"source":["# Dropout forward pass\n","Dropout 是神經網路的一種標準化 (regularization) 方式，在 forward pass 的流程中會將前一層輸出結果的其中一些數值隨機重設為零 [1] 。\n","\n","[1] [Geoffrey E. Hinton et al, \"Improving neural networks by preventing co-adaptation of feature detectors\", arXiv 2012](https://arxiv.org/abs/1207.0580)\n","\n","請寫出 `layers.py` 檔案中的 `dropout_forward` 函數，然後執行下方 code 以做檢查。記得 dropout 在 training 和 testing 階段的運作模式不同。"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"PpvW5lwa6h2i","executionInfo":{"status":"ok","timestamp":1686984287664,"user_tz":-480,"elapsed":722,"user":{"displayName":"Kevin Tsai","userId":"03189192575775510032"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4fc60f00-ac2a-4f55-e43a-d081a7fd8ffd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running tests with p =  0.25\n","Mean of input:  10.000207878477502\n","Mean of train-time output:  10.014059116977283\n","Mean of test-time output:  10.000207878477502\n","Fraction of train-time output set to zero:  0.749784\n","Fraction of test-time output set to zero:  0.0\n","\n","Running tests with p =  0.4\n","Mean of input:  10.000207878477502\n","Mean of train-time output:  9.977917658761159\n","Mean of test-time output:  10.000207878477502\n","Fraction of train-time output set to zero:  0.600796\n","Fraction of test-time output set to zero:  0.0\n","\n","Running tests with p =  0.7\n","Mean of input:  10.000207878477502\n","Mean of train-time output:  9.987811912159426\n","Mean of test-time output:  10.000207878477502\n","Fraction of train-time output set to zero:  0.30074\n","Fraction of test-time output set to zero:  0.0\n","\n"]}],"source":["np.random.seed(231)\n","x = np.random.randn(500, 500) + 10\n","\n","for p in [0.25, 0.4, 0.7]:\n","  out, _ = dropout_forward(x, {'mode': 'train', 'p': p})\n","  out_test, _ = dropout_forward(x, {'mode': 'test', 'p': p})\n","\n","  print('Running tests with p = ', p)\n","  print('Mean of input: ', x.mean())\n","  print('Mean of train-time output: ', out.mean())\n","  print('Mean of test-time output: ', out_test.mean())\n","  print('Fraction of train-time output set to zero: ', (out == 0).mean())\n","  print('Fraction of test-time output set to zero: ', (out_test == 0).mean())\n","  print()"]},{"cell_type":"markdown","metadata":{"id":"-2DB4u836h2i"},"source":["## Inline Question 1:\n","Please explain why your results in the above cell makes sense.\n","\n","## Answer:\n","[FILL THIS IN]"]},{"cell_type":"markdown","metadata":{"id":"Z55i_0866h2i"},"source":["## Inline Question 2:\n","What happens if we do not divide the values being passed through dropout by `p` in the dropout layer? Why does that happen?\n","\n","## Answer:\n","[FILL THIS IN]"]},{"cell_type":"markdown","metadata":{"id":"v1eeqsT96h2i"},"source":["# Dropout backward pass\n","請完成同個檔案中的 `dropout_backward` 函數，然後執行下方 code 以檢查 gradient 數值。"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"lTEiMJyX6h2i","executionInfo":{"status":"ok","timestamp":1686985089614,"user_tz":-480,"elapsed":334,"user":{"displayName":"Kevin Tsai","userId":"03189192575775510032"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fbdd4e3b-84a9-43c7-b170-221d84dec345"},"outputs":[{"output_type":"stream","name":"stdout","text":["dx relative error:  5.44560814873387e-11\n"]}],"source":["np.random.seed(231)\n","x = np.random.randn(10, 10) + 10\n","dout = np.random.randn(*x.shape)\n","\n","dropout_param = {'mode': 'train', 'p': 0.2, 'seed': 123}\n","out, cache = dropout_forward(x, dropout_param)\n","dx = dropout_backward(dout, cache)\n","dx_num = eval_numerical_gradient_array(lambda xx: dropout_forward(xx, dropout_param)[0], x, dout)\n","\n","# Error should be around e-10 or less\n","print('dx relative error: ', rel_error(dx, dx_num))"]},{"cell_type":"markdown","metadata":{"id":"E4ZmeXDh6h2i"},"source":["# Fully-connected nets with Dropout\n","我們在 `classifiers/fc_net.py` 檔案裡已經用 modular design 幫大家組合出一個更強大的 `FullyConnectedNet` 神經網路。這個網路的架構是：\n","\n","`{affine - batch/layer norm - relu - dropout} x (L - 1) - affine - softmax`\n","\n","我們可以任意決定網路 layer 的數目 $L$。這裡使用的 relu 和 dropout layers 是大家上面完成的模組。網路的 constructor 用法如下：\n","\n","```python\n","FullyConnectedNet(hidden_dims, input_dim, num_classes, dropout, normalization,\n","                  reg, weight_scale, dtype, seed)\n","```\n","\n","`hidden_dims` 儲存網路所有 hidden (affine) layers 的大小，所以 `len(hidden_dims) = L`。`dropout` 決定 dropout layers 的 `p` 參數，如果 `dropout` 為一，我們就略過 dropout layers。\n","\n","請詳讀並執行以下 code 以了解 `FullyConnectedNet` 的使用方式。"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"7aucMXY06h2i","executionInfo":{"status":"ok","timestamp":1686985096992,"user_tz":-480,"elapsed":4375,"user":{"displayName":"Kevin Tsai","userId":"03189192575775510032"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2f08e7e1-0ca1-4f92-9b58-792b77e9e2b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running check with dropout =  1\n","Initial loss:  2.3004790897684924\n","W1 relative error: 1.48e-07\n","W2 relative error: 2.21e-05\n","W3 relative error: 3.53e-07\n","b1 relative error: 5.38e-09\n","b2 relative error: 2.09e-09\n","b3 relative error: 5.80e-11\n","\n","Running check with dropout =  0.75\n","Initial loss:  2.302371489704412\n","W1 relative error: 1.90e-07\n","W2 relative error: 4.76e-06\n","W3 relative error: 2.60e-08\n","b1 relative error: 4.73e-09\n","b2 relative error: 1.82e-09\n","b3 relative error: 1.70e-10\n","\n","Running check with dropout =  0.5\n","Initial loss:  2.3042759220785896\n","W1 relative error: 3.11e-07\n","W2 relative error: 1.84e-08\n","W3 relative error: 5.35e-08\n","b1 relative error: 5.37e-09\n","b2 relative error: 2.99e-09\n","b3 relative error: 1.13e-10\n","\n"]}],"source":["np.random.seed(231)\n","N, D, H1, H2, C = 2, 15, 20, 30, 10\n","X = np.random.randn(N, D)\n","y = np.random.randint(C, size=(N,))\n","\n","for dropout in [1, 0.75, 0.5]:\n","  print('Running check with dropout = ', dropout)\n","  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n","                            weight_scale=5e-2, dtype=np.float64,\n","                            dropout=dropout, seed=123)\n","\n","  loss, grads = model.loss(X, y)\n","  print('Initial loss: ', loss)\n","\n","  # Relative errors should be around e-6 or less; Note that it's fine\n","  # if for dropout=1 you have W2 error be on the order of e-5.\n","  for name in sorted(grads):\n","    f = lambda _: model.loss(X, y)[0]\n","    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n","    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))\n","  print()"]},{"cell_type":"markdown","metadata":{"id":"xqccQRVe6h2i"},"source":["# Regularization experiment\n","\n","現在大家來做個實驗！我們先建立兩套 two-layer network，第一個不使用 dropout，而第二個則隨機保留 ~25% 的神經元。我們接著使用 CIFAR-10 的一部分資料做訓練，並觀測兩種網路的 training 和 validation accuracy。這裡的 training 會用到我們提供的 Solver 套件，Solver 可以指定模型學習的模式（SGD、SGD + momentum、RMSProp 和 Adam）和相關的 hyperparameters，用法請參考以下 code 。"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"YcrkYi-z6h2j","executionInfo":{"status":"aborted","timestamp":1686984240027,"user_tz":-480,"elapsed":6,"user":{"displayName":"Kevin Tsai","userId":"03189192575775510032"}}},"outputs":[],"source":["# Train two identical nets, one with dropout and one without\n","np.random.seed(231)\n","num_train = 500\n","small_data = {\n","  'X_train': data['X_train'][:num_train],\n","  'y_train': data['y_train'][:num_train],\n","  'X_val': data['X_val'],\n","  'y_val': data['y_val'],\n","}\n","\n","solvers = {}\n","dropout_choices = [1, 0.25]\n","for dropout in dropout_choices:\n","  model = FullyConnectedNet([500], dropout=dropout)\n","  print('Dropout: ', dropout)\n","\n","  solver = Solver(model, small_data,\n","                  num_epochs=25, batch_size=100,\n","                  update_rule='adam',\n","                  optim_config={\n","                    'learning_rate': 5e-4,\n","                  },\n","                  verbose=True, print_every=100)\n","  solver.train()\n","  solvers[dropout] = solver\n","  print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c6UrfGhs6h2j","executionInfo":{"status":"aborted","timestamp":1686984240027,"user_tz":-480,"elapsed":6,"user":{"displayName":"Kevin Tsai","userId":"03189192575775510032"}}},"outputs":[],"source":["# Plot train and validation accuracies of the two models\n","\n","train_accs = []\n","val_accs = []\n","for dropout in dropout_choices:\n","  solver = solvers[dropout]\n","  train_accs.append(solver.train_acc_history[-1])\n","  val_accs.append(solver.val_acc_history[-1])\n","\n","plt.subplot(3, 1, 1)\n","for dropout in dropout_choices:\n","  plt.plot(solvers[dropout].train_acc_history, 'o', label='%.2f dropout' % dropout)\n","plt.title('Train accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend(ncol=2, loc='lower right')\n","\n","plt.subplot(3, 1, 2)\n","for dropout in dropout_choices:\n","  plt.plot(solvers[dropout].val_acc_history, 'o', label='%.2f dropout' % dropout)\n","plt.title('Val accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend(ncol=2, loc='lower right')\n","\n","plt.gcf().set_size_inches(15, 15)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"tags":["pdf-inline"],"id":"SFJ-CcBs6h2j"},"source":["## Inline Question 3:\n","Compare the validation and training accuracies with and without dropout -- what do your results suggest about dropout as a regularization technique?\n","\n","## Answer:\n","[FILL THIS IN]\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}